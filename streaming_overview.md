# Stream computing overview

EnOS™ Stream Computing targets to meet the real-time processing requirements.

Powered by Apache Spark™ Streaming with Envision’s customization and optimization, the EnOS stream data processing engine offers high scalability, high throughput, and high fault-tolerance.

The stream computing service provides access to the abstract computing models of user devices and sites. You can run your codes through the SDKs/APIs provided by EnOS™ for flexible customization based on your business needs.

## What is streaming data?

Broadly speaking, the generation of data can be considered as a series of discrete events. When drawing these discrete events on a time axis, an event stream or data stream is formed. The streaming data is consisted by these endless event streams.

Both offline data and streaming data are normally sent as logs. Unlike traditional offline data, streaming data is the data that is generated continuously by a lot of data sources. However, the size of the streaming data is normally smaller than the offline data.

The common sources of streaming data can be the equipments connected to a data center, the telemetry data of devicse, and the log files generated by mobile or Web applications.

## What is stream computing?

In general, stream computing has the following characteristics:

**Real-time and unbounded data streaming**

The computation faced by the engine is real-time and streaming, and the data streams are subscribed and consumed by stream computing in chronological order.

Because the data is generated continuously, the data stream is integrated to the streaming system continuously. For example, the website access log is a type of streaming data, the log continuously records the data as long as the website is alive. Thus, the streaming data is always the real-time and unbounded data.

**Continuous and efficient computation**

The computation models of stream computing is "event triggered". The trigger is the unbounded streaming data mentioned in the above section. Once the new streaming data is sent to system, the system immediately initiates and performs a computation task. Therefore, the stream computing is a continuous process.

**Streaming and real-time data integration**

The result of stream computing triggered by streaming data is recorded directly into the destination data storage. For example, the data can be directly written into the relational database (RDS) for report rendering. Therefore, the computing results of the streaming data is continuously recorded into the target data storage.

## When is stream computing needed?

Stream computing can be used for the following scenarios:

- Computation of device states

  In some business scenarios, you might need to obtain certain state parameters of a device to confirm the status of a device, the system.

  You might want to program under the following circumstances:
  * The original acquisition points do not directly provide the needed data.
  * The business scenario is relatively complex and the values of multiple acquisition points need to be considered comprehensively.

- Aggregating and calculating the site information

## Considerations for stream computing

- All types defined in the computation package must be serializable, otherwise the computation package will fail to be loaded.

- The stream computing framework maintains the state of devices and sites internally. The information is updated in real time according to the acquired data.

  System updates both the acquisition point value and the device connection status to the latest state. The update frequency depends on the acquisition and upload frequency of the acquisition point.

- If a device is not connected, it means that the acquisition point of the device is not updated.

- At present, the frequency of computing from device-level and site-level both are 5 seconds. Therefore, even if the user data refreshes every second, the computation package is still triggered every 5 seconds. In future version, a customizable computing frequency will be provided.

- Computation is triggered only if the computation package has the appropriate data access rights.  

## Key concepts of EnOS™ Stream Computing

The following is the key concepts that use to develop a customized stream computing:

- **Batch Size**: Apache Spark™ Streaming is a stream computing engine based on micro-batch model and processes data at the interval of batch size, which determines how often a stream computing task gets the data.

- **Point Calculator**: The abstraction of the raw data acquisition points and user-defined computation points.

- **Device Calculator**: Calculation is based on device. For example, the device calculator can compute average wind speed of a specific interval based on the data of original wind speed measurement point.

- **Site Calculator**: Calculation is based on site, where a site is a logical concept which indicates a collection of multiple devices. For example, a collection of 10 turbines. The site calculator can compute site-level data, such as the total power generation for the 10 turbines of the logical site.

- **Virtual Calculator**: For logical sites, some acquisition points may be inconvenient to map to “any physical” devices, or some statistical points may be computed based on the sites and stored in a virtual device

- **Virtual Point**: Provided in the SDK for stream computing, used for representing the computation points of users.

## Data processing flow of EnOS™ Stream Computing

The procedure is as follows:

1. Processing of raw data

   Original acquisition points are sent to Kafka through the EnOS connection layer. The messages received are analysed by the stream computing process of acquisition points and are mapped into domain points. In this process, Site ID and other data of the device is added to the point information. The information is then output to the Redis real-time database. The data of each collecting point is also sent to Kafka at the same time.

2. Invocation of device-level computing packages

   In this step, the device calculator states are recorded, and the user-defined computing based on stream data is performed. The computing process subscribes to the messages previously output to Kafka, that is, gets the data from original acquisition points, maps the acquisition points to point calculators, and aggregates the point calculators under each device calculator.

   The device calculators will then traverse the loaded computing packages based on updated state of the acquisition points and verify permissions based on the information provided in computing packages. The points computed by users are returned to platform framework and output to Redis real-time database and Kafka.

3. Invocation of site-level computing packages

   For devices under the same logical site, the platform framework aggregates the devices under site calculator object. Similar to the device-level computing process, the user’s computing packages need to pass permission verification to trigger the site-level computing. When the user or application passes the permission verification, the user or application can access all acquisition points under a site through the site calculator object.

4. Output of computing

   The data from the stream module flows into Redis and Kafka, and the downstream continues to subscribe all data from Kafka and record them to time series database (TSDB) and Hadoop distributed file system (HDFS). By default, data of the latest three months are reserved in TSDB to support real-time queries through the EnOS™ API, and the data in HDFS is used as the data source for offline data processing and further processing and analysis.
