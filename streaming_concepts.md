# Concepts

This article introduces the major concepts involved in stream data analytics.

## Stream data

Broadly speaking, the generation of data can be considered as a series of discrete events. When drawing these discrete events on a time axis, an event stream or data stream is formed. Streaming data consist of these endless event streams.

Both offline data and streaming data are normally sent as logs. Unlike traditional offline data, streaming data are generated continuously by a lot of data connection. However, the size of the streaming data is normally smaller than that of the offline data.

The common sources of streaming data can be the devices connected to a data center, the telemetry data of devices, and the log files generated by mobile or web applications.

## Stream analytics

In general, stream analytics has the following characteristics:

- Real-time and unbounded data streaming: The computation the engine processes is real-time and streaming, and the data streams are subscribed and consumed by stream computing in chronological order. Because data are generated continuously, the data streams are integrated to the streaming system continuously. For example, website access log is a type of streaming data, the log continuously records data as long as the website is online. Thus, the streaming data are always real-time and unbounded.
- Continuous and efficient computation: The computation models of stream computing are “event triggered”. The trigger is the unbounded streaming data mentioned in the previous section. Once new streaming data are sent to the system, the system immediately initiates and performs a computation task. Therefore, stream computing is a continuous process.
- Streaming and real-time data integration: The result of stream computing triggered by streaming data is recorded directly into the destination data storage. For example, the data can be directly written into the relational database (RDS) for report rendering. Therefore, the computing results of the streaming data are continuously recorded into the target data storage.

## Data type

EnOS Stream Processing Engine supports multiple data types with specific data processing templates. The data type of a measure point telemetry is defined when configuring the device model.

## Data processing strategy

Unified window aggregation scheme for the data of the same device model, which consists of an input point, output point, threshold, interpolation strategy, algorithm, and window size.

## Event time

In general, event time (time when an event occurs) is used in AI data aggregation. More precisely, every event has a corresponding timestamp, and the timestamp is part of the data record. Event time is actually a timestamp. When event time is used to define time windows, the stream processing engine can deal with disorderly time flow and variable event time deviation, and compute meaningful results based on the actual time of events.

## Time window

EnOS Stream Processing Engine is based on time windows (micro-batch model) and processes data at the specified window size (interval of batch size). Windowing is simply the notion of taking a data connection and chopping it up along temporal boundaries into finite chunks for processing, which determines how often a stream computing task gets the data. EnOS Streaming Processing Engine supports tumbling window.

## Tumbling window

Tumbling windows have a fixed size and do not overlap. Data belonging to a window will be aggregated with the specified method. For example, if you specify a tumbling window with a size of 5 minutes, the current window will be evaluated, and a new window will be started every five minutes. See the example in the following figure.

.. image:: media/window_type.png


## Window latency

Generally, when a window closes, the data processing in the window should have been completed, and a new window will be started. However, device data transfer might be delayed because of various factors like device failure and transmission efficiency. Latency setting is introduced to specify the extended validity time of the window after it closes. Late data arriving within the allowed lateness will be added to the window and computed again. Late data arriving after the allowed lateness will be ignored. In the following example, window size is 5 minutes, and window2 contains data falling in the time range of 11:00 - 11:05. If latency is not enabled, window2 will be closed at 11:05, and data1 and data2 that are arriving late will be ignored. If a latency of 5 minutes is enabled, data1 will be added to window2 for computing again, but data2 will be ignored.

.. image:: media/latency_setting.png



<!--end-->
